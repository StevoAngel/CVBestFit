{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee7a83f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo de Ollama cargado correctamente.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import spacy\n",
    "import torch\n",
    "import logging\n",
    "import pdfplumber\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from langdetect import detect\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9bef7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directrorios de los documentos requeridos:\n",
    "cvsPath = r'C:\\Users\\Estiven Angel\\NLP_p2025\\CVBestFit\\cvs' # Ruta donde se almacenan los CVs sin procesar\n",
    "#cvsTextPath = r'C:\\Users\\Estiven Angel\\NLP_p2025\\CVBestFit\\cvsNormalizedText' # Ruta donde se almacenan los CVs procesados\n",
    "trainingSetPath = r'C:\\Users\\Estiven Angel\\NLP_p2025\\CVBestFit\\cvsTrainingSet\\all' # Ruta del set de entrenamiento del modelo\n",
    "jobDescriptionFile = r'C:\\Users\\Estiven Angel\\NLP_p2025\\CVBestFit\\jobDescriptions\\DP - Asesor Planeaci√≥n SOP.pdf' # Directorio del documento descriptivo\n",
    "\n",
    "#os.makedirs(cvsTextPath, exist_ok=True) # Crear directorio en caso de que no exista "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af6dfd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fuciones de soporte:\n",
    "def loadModels(useGpu=True):\n",
    "    \"\"\" Carga el modelo de spaCy para espa√±ol e ingl√©s para GPU s√≠ hay alguna disponible, caso contrario: carga modelos de CPU.\"\"\"\n",
    "    # Se verifica si Torch detecta la GPU:\n",
    "    gpuAvailable = torch.cuda.is_available()\n",
    "\n",
    "    if useGpu and gpuAvailable:\n",
    "        try:\n",
    "            spacy.require_gpu()  # Forzar SpaCy a usar GPU si est√° disponible\n",
    "            modelEs = spacy.load(\"es_dep_news_trf\")\n",
    "            modelEn = spacy.load(\"en_core_web_trf\")\n",
    "\n",
    "            print(f\"‚úÖ GPU Disponible para spaCy: {torch.cuda.get_device_name(0)} üñ•Ô∏è\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"‚ö†Ô∏è  Error al cargar modelos con GPU. Se cargaron modelos del CPU.\")\n",
    "            print(\"Detalles del error:\", str(e))\n",
    "            modelEs = spacy.load(\"es_core_news_md\")\n",
    "            modelEn = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "    else:\n",
    "        print(\"‚ùå No GPU activada o disponible para spaCy. Se utilizar√° CPU.\")\n",
    "        modelEs = spacy.load(\"es_core_news_md\")\n",
    "        modelEn = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "    return modelEs, modelEn\n",
    "\n",
    "def chooseModel(text):\n",
    "    \"\"\" Detecta el idioma del texto y devuelve el modelo spaCy correspondiente ('es' o 'en').\"\"\"\n",
    "    language = detect(text)\n",
    "\n",
    "    if language == \"es\":\n",
    "        model = modelEs # Seleccionar modelo en espa√±ol\n",
    "    elif language == \"en\":\n",
    "        model = modelEn # Seleccionar modelo en ingles\n",
    "    else:\n",
    "        raise Exception(f\"Idioma no soportado: {language}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "def extractText(docPath):\n",
    "    \"\"\" Extrae y concatena el texto de todas las p√°ginas de un archivo PDF.\"\"\"\n",
    "    with pdfplumber.open(docPath) as pdf:\n",
    "        return \"\\n\".join([page.extract_text() or '' for page in pdf.pages])\n",
    "    \n",
    "def normalizeText(text):\n",
    "    \"\"\" Normaliza el texto eliminando stopwords y signos de puntuaci√≥n, y devuelve los lemas en min√∫sculas \"\"\"\n",
    "    model = chooseModel(text) # Seleccionar el modelo en base al idioma\n",
    "    doc = model(text) # Tokenizaci√≥n y POS Tagging\n",
    "\n",
    "    # Convertir a min√∫sculas, remover stop-words y signos de puntuaci√≥n\n",
    "    lemmas = [token.lemma_.lower() for token in doc if not token.is_stop and not token.is_punct]\n",
    "    return \" \".join(lemmas)\n",
    "\n",
    "def concatAllTexts(textPath):\n",
    "    \"\"\" Concatena el texto de todos los archivos .pdf en un directorio dado.\"\"\"\n",
    "    allTexts = []\n",
    "    for file in os.listdir(textPath):\n",
    "        if file.endswith('.pdf'): # Solo archivos PDF\n",
    "            cPath = os.path.join(textPath, file)\n",
    "            text = extractText(cPath) # Extraer el texto del documento actual\n",
    "            allTexts.append(text)\n",
    "\n",
    "        ######################## Suprimir avisos de pdfplumber sobre cropbox ############################\n",
    "        logging.getLogger(\"pdfminer\").setLevel(logging.ERROR)\n",
    "\n",
    "    return allTexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56eebb1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se define la funci√≥n de vectorizaci√≥n del corpus:\n",
    "def tfIdfWeightedEmbedding(texts, modelW2V):\n",
    "    \"\"\"Vectoriza el corpus de textos usando TF-IDF y calcula los embeddings ponderados de cada documento.\"\"\"\n",
    "    # Se vectorizan con TF-IDF los textos (documentos)\n",
    "    vectorizerTfIdf = TfidfVectorizer()\n",
    "    tfIdfVectors = vectorizerTfIdf.fit_transform(texts)\n",
    "    uniqueWords = vectorizerTfIdf.get_feature_names_out()\n",
    "\n",
    "    # Se mapean los pesos TF-IDF:\n",
    "    tfIdfWeights = []\n",
    "    for doc in tfIdfVectors:\n",
    "        wordWeight = {uniqueWords[i]: doc[0, i] for i in doc.nonzero()[1]}\n",
    "        tfIdfWeights.append(wordWeight)\n",
    "\n",
    "    # Se calculan los embeddings ponderados de cada documento:\n",
    "    embedings = {}\n",
    "\n",
    "    for i, doc in enumerate(texts):\n",
    "        weights = [] # Pesos TF-IDF\n",
    "        vectors = [] # Vectores ponderados de las palabras encontradas\n",
    "        tokens = doc.split() # Palabras tokenizadas\n",
    "\n",
    "        for word in tokens:\n",
    "            if word in modelW2V.wv and word in tfIdfWeights[i]:\n",
    "                vectors.append(modelW2V.wv[word] * tfIdfWeights[i][word]) # Se guarda el vector ponderado\n",
    "                weights.append(tfIdfWeights[i][word]) # Se guarda el peso TF-IDF de la palabra\n",
    "\n",
    "        if vectors: # S√≠ hay palabras con word embeddings y TF-IDF:\n",
    "            embedings[i] = np.sum(vectors, axis=0) / sum(weights) # Calcular el promedio ponderado de los vectores\n",
    "        else: # S√≠ no:\n",
    "            embedings[i] = np.zeros(modelW2V.vector_size) # Regresar un vector de ceros para ese documento\n",
    "\n",
    "    return embedings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff2fda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar el modelo de spacy para procesar el texto\n",
    "modelEs, modelEn = loadModels() # Cargar los modelos para espa√±ol e ingl√©s\n",
    "\n",
    "# Se entrena el modelo Word2Vec con todos los CVs y la descripci√≥n de la vacante:\n",
    "\n",
    "# Almacenar los CVs:\n",
    "trainingTexts = concatAllTexts(trainingSetPath) # Textos concatenados para entrenamiento\n",
    "rawTexts = concatAllTexts(cvsPath) # Texto de los CVs de la vacante actual\n",
    "\n",
    "# Almacenar tambi√©n la descripci√≥n del trabajo para el entrenamiento:\n",
    "jobDescription = extractText(jobDescriptionFile)\n",
    "trainingTexts.append(jobDescription)\n",
    "rawTexts.append(jobDescription)\n",
    "\n",
    "# Se tokenizan los textos de trainingTexts para el entrenamiento del modelo:\n",
    "tokenizedTrainingText = [text.lower().split() for text in trainingTexts] # Se convierten tambi√©n a min√∫sculas\n",
    "\n",
    "# Se entrena el modelo word2Vec:\n",
    "modelW2V = Word2Vec(sentences=tokenizedTrainingText, vector_size=100, min_count=1, sg=1)\n",
    "\n",
    "# No nOrmalizar...\n",
    "# Seccionar en xml los cvs\n",
    "# Comparar los candidatos entre s√≠.\n",
    "# Revisar la opci√≥n de chunks\n",
    "# Lexicons para el nombre\n",
    "\n",
    "# Extraer embeddings ponderados de los CVs y descripciones de la vacante actual:\n",
    "#normalizedTexts = [normalizeText(text) for text in rawTexts] # Normalizar el texto\n",
    "tokenizedRawTexts = [text.lower() for text in rawTexts]\n",
    "embeddings = tfIdfWeightedEmbedding(tokenizedRawTexts, modelW2V) # Extraer TF-IDF embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71dabccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "jobDescriptionVec = embeddings[len(embeddings)-1]  # El √∫ltimo vector ponderado corresponde a la descripci√≥n de la vacante\n",
    "\n",
    "scores = {} # Se almacenan los scores (similitud coseno) de cada CV\n",
    "\n",
    "for i, file in enumerate(os.listdir(cvsPath)):\n",
    "    if file.endswith('.pdf'):\n",
    "        cosSim = cosine_similarity([jobDescriptionVec], [embeddings[i]]).item() # Calcular similitud del CV contra la descripci√≥n\n",
    "        nameCV = os.path.splitext(file)[0] # Nombre del CV\n",
    "\n",
    "        scores[nameCV] = cosSim # Se guarda el score como valor y el nombre del CV como clave\n",
    "\n",
    "    ######################## Suprimir avisos de pdfplumber sobre cropbox ############################\n",
    "    logging.getLogger(\"pdfminer\").setLevel(logging.ERROR)\n",
    "\n",
    "# Se ordenan los scores de manera descendente:\n",
    "scores = dict(sorted(scores.items(), key=lambda item: item[1], reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a6d885",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graficar los resultados de los scores:\n",
    "scoresDF = pd.DataFrame(list(scores.items()), columns=['CV', 'Puntuaci√≥n']) # Crear un DataFrame\n",
    "display(scoresDF)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "sns.barplot(x='CV', y='Puntuaci√≥n', data=scoresDF, hue='CV', palette='Spectral',ax=ax)\n",
    "plt.title(\"√çndice de afinidad Candidato-Vacante\")\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.5)\n",
    "plt.xticks(rotation=90)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d637afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear heur√≠stica para puntuar cada skill con la que cuente el candidato:\n",
    "def getSkillVector(skill, modelW2V):\n",
    "    \"Obtiene el vector promedio de todas las palabras de la skill\"\n",
    "    skillVector = np.mean([modelW2V.wv[word] for word in skill if word in modelW2V.wv.key_to_index], axis=0)\n",
    "    return skillVector\n",
    "\n",
    "def getSkillSynonyms(allSkills, corpus, modelW2V, threshold=0.7):\n",
    "    \"\"\" Devuelve un diccionario de los sinonimos de la skill (palabra o frase) mediante similitud vectorial\"\"\"\n",
    "    synonyms = {}\n",
    "    flatCorpus = [word for sentence in corpus for word in sentence] # Todas las palabras del corpus en una sola lista\n",
    "\n",
    "    for skill in allSkills:\n",
    "        similarSkills = []\n",
    "        skillVector = getSkillVector(skill, modelW2V)\n",
    "\n",
    "        for word in flatCorpus:\n",
    "            if word in modelW2V.wv.key_to_index:\n",
    "                try:\n",
    "                    cosSim = modelW2V.wv.cosine_similarities(skillVector, [modelW2V.wv[word]])[0]\n",
    "                    if cosSim > threshold:\n",
    "                        similarSkills.append(word) # Guardar el sin√≥nimo s√≠ excede el threshold\n",
    "\n",
    "                except KeyError:\n",
    "                    continue\n",
    "            \n",
    "        synonyms[skill] = [skill] # Agregar la propia skill al diccionario\n",
    "        synonyms[skill].extend(similarSkills) # Agregar las skills similares al diciconario\n",
    "\n",
    "    return synonyms\n",
    "\n",
    "def evaluateSkills(skills, weight, cvSkills, synonyms):\n",
    "        score = 0\n",
    "        total = 0\n",
    "\n",
    "        for skill in skills:\n",
    "            total += weight\n",
    "            found = any(syn in cvSkills for syn in synonyms[skill])\n",
    "            score += weight if found else -weight\n",
    "        \n",
    "        return score, total\n",
    "\n",
    "def skillsScore(cvSkills, hardSkillsMust, hardSkillsNice, softSkillsMust, softSkillsNice):\n",
    "    \"\"\" Eval√∫a con una heur√≠stica la puntuaci√≥n por cada skill con la que cuente el CV y devuelve dicha puntuaci√≥n\"\"\"\n",
    "\n",
    "    # Enriquecer las skills con sin√≥nimos:\n",
    "    allSkills = cvSkills + hardSkillsMust + hardSkillsNice + softSkillsMust + softSkillsNice\n",
    "    synonyms = getSkillSynonyms(allSkills, tokenizedTrainingText, modelW2V)\n",
    "    \n",
    "    # synonymsDF = pd.DataFrame(list(synonyms.items()), columns=['Palabra', 'Sin√≥nimos']) # Crear un DataFrame\n",
    "    # display(synonymsDF)  \n",
    "\n",
    "    # Definir los pesos de las skills:\n",
    "    weights = {\n",
    "        \"hardMust\": 5,\n",
    "        \"hardNice\": 3,\n",
    "        \"softMust\": 2,\n",
    "        \"softNice\": 1\n",
    "    }\n",
    "\n",
    "    # Evaluar las categor√≠as de skills:\n",
    "    categories = [\n",
    "        (hardSkillsMust, weights[\"hardMust\"]),\n",
    "        (hardSkillsNice, weights[\"hardNice\"]),\n",
    "        (softSkillsMust, weights[\"softMust\"]),\n",
    "        (softSkillsNice, weights[\"softNice\"]),\n",
    "    ]\n",
    "\n",
    "    score = 0\n",
    "    totalPossible = 0\n",
    "\n",
    "    for skillList, weight in categories:\n",
    "        if skillList:\n",
    "            subScore, subTotal = evaluateSkills(skillList, weight, cvSkills, synonyms)\n",
    "            score += subScore\n",
    "            totalPossible += subTotal\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    # print(f\"Total posible: {totalPossible}\")\n",
    "    # print(f\"Score bruto: {score}\")\n",
    "\n",
    "    skillsScore = max(score, 0) / totalPossible if totalPossible else 0\n",
    "\n",
    "    return skillsScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6dcad6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cvSkills = [\"R, SQL, Python\"]\n",
    "hardSkillsMust = [\"Calibracion de sensores, electroneumatica, Conocimiento en PLCs como Siemens o Allen Bradley\"]\n",
    "hardSkillsNice = [\"Pandas, Scikit-learn, Tensorflow, Pytorch, and OpenCV\"]\n",
    "softSkillsMust = [\"Big Data, Feature Engineering, optimizaci√≥n, machine learning\"]\n",
    "softSkillsNice = [\"Apache Spark, Amazon Web Services, Google Cloud\"]\n",
    "\n",
    "skillScore = skillsScore(cvSkills, hardSkillsMust, hardSkillsNice, softSkillsMust, softSkillsNice)\n",
    "print(skillScore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae625d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
